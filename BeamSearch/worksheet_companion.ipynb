{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uOHHZ0vFbTz"
      },
      "source": [
        "# Generating Text with Beam Search and GPT-2\n",
        "This notebook will help you to perform beam search on your worksheet using GPT-2. In particular, `next_token_probs` will give you the `n_tokens` tokens with the highest probability given some input, along with the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14xQobyC2hLB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFAoR4p-4tWO"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedModel\n",
        "\n",
        "\n",
        "def next_token_probs(prompt_tokens: List[int], model: PreTrainedModel,\n",
        "                     n_tokens: int, log: bool = False) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Get the n tokens with the highest probability along with their probability\n",
        "    from a pytorch generation model\n",
        "\n",
        "    Args:\n",
        "        prompt_tokens (List[int]): the tokens from the prompt\n",
        "        model (PreTrainedModel): the model\n",
        "        n_tokens (int): the number of tokens to return\n",
        "        log (bool): use log softmax. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: a dictionary mapping token IDs to probabilities\n",
        "    \"\"\"\n",
        "    logits = model(torch.LongTensor(prompt_tokens)).logits\n",
        "    # this computes the softmax of the logits.\n",
        "    # the first index, 0, is for the batch\n",
        "    # the second index, -1, is for the last token\n",
        "    if log:\n",
        "        next_token_probs = torch.log_softmax(logits[-1, :], 0)\n",
        "    else:\n",
        "        next_token_probs = torch.softmax(logits[-1, :], 0)\n",
        "    # this sorts the probabilities and returns the INDICES of the probabilities\n",
        "    # in descending sorted order\n",
        "    sorted_token_ids = torch.argsort(next_token_probs, descending=True)\n",
        "    top_token_ids = [tok_id.item() for tok_id in sorted_token_ids[:n_tokens]]\n",
        "    # convert to a dictionary mapping token IDs to probabilities\n",
        "    return dict(zip(top_token_ids, next_token_probs[top_token_ids].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qafN4xWlcEQb"
      },
      "source": [
        "This will generate the first two tokens, which are given to you on the worksheet. Note the lack of a space before `\",\"`! That means that the next two prompts should be:\n",
        "* `\"The sun is shining on\"`\n",
        "* `\"The sun is shining,\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpgV4n6iCzss"
      },
      "outputs": [],
      "source": [
        "most_probable_tokens = next_token_probs(tokenizer.encode(\"The sun is shining\"),\n",
        "                                        model, 2, log=False)\n",
        "\n",
        "for token_id, prob in most_probable_tokens.items():\n",
        "  token = tokenizer.decode(token_id)\n",
        "  print(f'Token ID: {token_id}, Token: \"{token}\", Prob: {prob}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
