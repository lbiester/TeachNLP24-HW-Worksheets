# Report: Beam Search for Text Generation
## Generation with GPT-2
Write your own test case for your beam search method. You should:

1. Ensure that the results differ from the results you get from greedy decoding with the same number of tokens.
2. Compare your results to the results returned by the `generator` function, as is done in `test_mini.py`, to confirm that your function works correctly.

### Write the details of your test case:
* Prompt:
* Beam Width:
* Max New Tokens:

### Do you prefer the text that was generated by greedy decoding or beam search? Why?

## Evaluation of Machine Translation
Run the `evaluate_bleu.py` script. To run it, run

```bash
python evaluate_bleu.py LAGUAGE
```

on ada, replacing `LANGUAGE` with your language of your choice (the options are listed in the assignment).

The script prints the source sentence and two versions of the translation and asks you to state which translation is better. After you've seen all of the examples, it prints the algorithm used for each translation and the BLEU scores. Important note: some of the sentences are a bit weird - unfortunately I don't have time to filter all of them.

### Your results
Copy the results of your script after the line `EVALUATION SUMMARY` here:




### Did you find that you tended to prefer the beam search or greedy search translations? Did you agree or disagree with the BLEU metric? Why?
_Write at least two sentences. If you can, try to connect your findings to your knowledge about the decoding algorithms and BLEU score._



